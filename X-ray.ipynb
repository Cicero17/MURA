{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os \n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from csv import reader\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout \n",
    "from tensorflow.keras import models, layers, backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(source, kind, reshape, standardize):\n",
    "    train_imgs, train_vals = extract_helper(source, 'train', kind, reshape)\n",
    "    test_imgs, test_vals = extract_helper(source, 'valid', kind, reshape)\n",
    "    if standardize == True:\n",
    "        mean = np.mean(train_imgs, axis = (1,2), keepdims = True)\n",
    "        std = np.std(train_imgs, axis = (1,2), keepdims = True)\n",
    "        train_imgs = (train_imgs - mean)/std\n",
    "        test_imgs = (test_imgs - mean)/std\n",
    "    return train_imgs, train_vals, test_imgs, test_vals\n",
    "\n",
    "def extract_all_data(source, reshape, standardize):\n",
    "    train_imgs, train_vals = extract_all_helper(source, 'train', reshape)\n",
    "    test_imgs, test_vals = extract_all_helper(source, 'valid', reshape)\n",
    "    if standardize == True:\n",
    "        mean = np.mean(train_imgs, axis = (1,2), keepdims = True)\n",
    "        std = np.std(train_imgs, axis = (1,2), keepdims = True)\n",
    "        train_imgs = (train_imgs - mean)/std\n",
    "        test_imgs = (test_imgs - mean)/std\n",
    "    return train_imgs, train_vals, test_imgs, test_vals\n",
    "\n",
    "def extract_helper(source, torv, kind, reshape):\n",
    "    os.chdir(source+'\\\\MURA-v1.1')\n",
    "    os.chdir(torv+'_specific_paths')\n",
    "    file = open(torv+'_image_paths_'+kind+'.csv')\n",
    "    return extract(source, file, reshape)\n",
    "\n",
    "def extract_all_helper(source, torv, reshape):\n",
    "    os.chdir(source+'\\\\MURA-v1.1')\n",
    "    file = open(torv+'_image_paths.csv')\n",
    "    return extract(source, file, reshape)\n",
    "    \n",
    "def extract(source, file, reshape):\n",
    "    readCSV = reader(file)\n",
    "    imgs = []\n",
    "    vals = []\n",
    "    for row in readCSV:\n",
    "        im = cv2.imread(source+'\\\\'+row[0], cv2.IMREAD_GRAYSCALE)\n",
    "        imgs.append(np.array(cv2.resize(im,reshape)))\n",
    "        if 'positive' in row[0]:\n",
    "            vals.append(1)\n",
    "        else:\n",
    "            vals.append(0)\n",
    "    file.close()\n",
    "    imgs = np.array(imgs)\n",
    "    vals = np.array(vals)\n",
    "    imgs = np.expand_dims(imgs, axis=3)\n",
    "    return imgs,vals\n",
    "\n",
    "class patient:\n",
    "    def __init__(self, imgs, vals, value):\n",
    "        self.imgs = imgs\n",
    "        self.vals = vals\n",
    "        self.value = value\n",
    "        \n",
    "def patient_code(path):\n",
    "    pos = path.find('patient')+7\n",
    "    return path[pos:pos+5]\n",
    "\n",
    "def patient_value(path):\n",
    "    if 'positive' in path:\n",
    "        return 1\n",
    "    return 0\n",
    "        \n",
    "def extract_data_patients(source, kind, reshape, standardize):\n",
    "    train_patients = extract_helper_patients(source, 'train', kind, reshape)\n",
    "    test_patients  = extract_helper_patients(source, 'valid',  kind, reshape)\n",
    "    if standardize == True:\n",
    "        train_imgs = np.array([])\n",
    "        for p in train_patients:\n",
    "            train_imgs = np.concatenate(train_imgs, p.imgs)\n",
    "        mean = np.mean(train_imgs, axis = (1,2), keepdims = True)\n",
    "        std = np.std(train_imgs, axis = (1,2), keepdims = True)\n",
    "        for p in train_patients:\n",
    "            p.imgs = (p.imgs - mean)/std\n",
    "        for p in test_patients:\n",
    "            p.imgs = (p.imgs - mean)/std\n",
    "    return train_patients, test_patients\n",
    "    \n",
    "def extract_helper_patients(source, torv, kind, reshape):\n",
    "    os.chdir(source+'\\\\MURA-v1.1')\n",
    "    os.chdir(torv+'_specific_paths')\n",
    "    file = open(torv+'_image_paths_'+kind+'.csv')\n",
    "    return extract_patients(source, file, reshape)\n",
    "\n",
    "def extract_patients(source, file, reshape):\n",
    "    patients = []\n",
    "    readCSV = reader(file)\n",
    "    imgs = []\n",
    "    vals = []\n",
    "    \n",
    "    row = next(readCSV)\n",
    "    prev_patient = patient_code(row[0])\n",
    "    vals.append(patient_value(row[0]))\n",
    "    im = cv2.imread(source+'\\\\'+row[0], cv2.IMREAD_GRAYSCALE)\n",
    "    imgs.append(np.array(cv2.resize(im,reshape)))\n",
    "    \n",
    "    for row in readCSV:\n",
    "        curr_patient = patient_code(row[0])\n",
    "        if curr_patient == prev_patient:\n",
    "            vals.append(patient_value(row[0]))\n",
    "            im = cv2.imread(source+'\\\\'+row[0], cv2.IMREAD_GRAYSCALE)\n",
    "            imgs.append(np.array(cv2.resize(im,reshape)))\n",
    "                \n",
    "        else:\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = np.expand_dims(imgs, axis=3)\n",
    "            vals = np.array(vals)\n",
    "            patients.append(patient(imgs, vals, vals[0]))\n",
    "            imgs = []\n",
    "            vals = []\n",
    "            prev_patient = curr_patient\n",
    "            vals.append(patient_value(row[0]))\n",
    "            im = cv2.imread(source+'\\\\'+row[0], cv2.IMREAD_GRAYSCALE)\n",
    "            imgs.append(np.array(cv2.resize(im,reshape)))\n",
    "                \n",
    "    file.close()\n",
    "    imgs = np.array(imgs)\n",
    "    imgs = np.expand_dims(imgs, axis=3)\n",
    "    vals = np.array(vals)\n",
    "    patients.append(patient(imgs, vals, vals[0])) \n",
    "\n",
    "    return patients  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_validation(model, data_x, data_y, rate, batch_size, number_of_epochs, class_weights):\n",
    "    rate = int(len(data_y)*rate)\n",
    "    train_x, train_y = shuffler(data_x[:rate], data_y[:rate])\n",
    "    valid_x, valid_y = shuffler(data_x[rate:], data_y[rate:])\n",
    "    score = 0\n",
    "    if class_weights == True:\n",
    "        model_copy = copy_model(model)\n",
    "        model_copy.fit(train_x, train_y, batch_size = batch_size, epochs = number_of_epochs, class_weight = class_weight(train_y))\n",
    "        score = conf_matrix(model_copy, valid_x, valid_y)\n",
    "    else:\n",
    "        model.fit(train_x, train_y, batch_size = batch_size, epochs = number_of_epochs)\n",
    "        score = conf_matrix(model, valid_x, valid_y)\n",
    "    if class_weights == True:\n",
    "        model.fit(data_x, data_y, batch_size = batch_size, epochs = number_of_epochs, class_weight = class_weight(data_y))\n",
    "    else:\n",
    "        model.fit(valid_x, valid_y, batch_size = batch_size, epochs = number_of_epochs)\n",
    "    return score, model\n",
    "\n",
    "def k_fold_cross_validation(k, model, data_x, data_y, batch_size, number_of_epochs, class_weights):\n",
    "    data_x, data_y = shuffler(data_x, data_y)\n",
    "    folds_x = []\n",
    "    folds_y = []\n",
    "    l = len(data_y)\n",
    "    for i in range(k):\n",
    "        folds_x.append(data_x[(l//k)*i: (l//k)*(i+1)])\n",
    "        folds_y.append(data_y[(l//k)*i: (l//k)*(i+1)])\n",
    "    score = 0\n",
    "    for i in range(k):\n",
    "        model_copy = copy_model(model)\n",
    "        for j in range(k):\n",
    "            if j!=i:\n",
    "                if class_weights == True:\n",
    "                    model_copy.fit(folds_x[j],folds_y[j], batch_size = batch_size, epochs = number_of_epochs, class_weight = class_weight(folds_y[j]))\n",
    "                else:\n",
    "                    model_copy.fit(folds_x[j],folds_y[j], batch_size = batch_size, epochs = number_of_epochs)\n",
    "        score += model_copy.evaluate(folds_x[i],folds_y[i])[1]\n",
    "    \n",
    "    if class_weights == True:\n",
    "        model.fit(data_x, data_y, batch_size = batch_size, epochs = number_of_epochs, class_weight = class_weight(data_y))\n",
    "    else:\n",
    "        model = model_copy\n",
    "        model.fit(folds_x[k-1],folds_y[k-1], batch_size = batch_size, epochs = number_of_epochs)\n",
    "    return score/k, model\n",
    "\n",
    "def shuffler(data_x, data_y):\n",
    "    p = np.random.permutation(len(data_y))\n",
    "    return (data_x[p], data_y[p])\n",
    "\n",
    "def conf_matrix(model, data_x, data_y): \n",
    "    y_pred = model.predict(data_x).flatten().tolist()\n",
    "    y_true = data_y.tolist()\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = round(y_pred[i])\n",
    "    return print_conf_matrix(y_true, y_pred)\n",
    "\n",
    "def patients_conf_matrix(model, test_patients): \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for p in test_patients:\n",
    "        y_true.append(p.value)\n",
    "        p_predict = round(np.mean(model.predict(p.imgs)))\n",
    "        y_pred.append(p_predict)\n",
    "    return print_conf_matrix(y_true, y_pred)\n",
    "    \n",
    "def print_conf_matrix(y_true, y_pred):\n",
    "    score = 0\n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "    l = len(y_true)\n",
    "    for i in range(l):\n",
    "        if y_pred[i] == 0 and y_true[i] == 0:\n",
    "            tn+= 1\n",
    "            score+= 1\n",
    "        elif y_pred[i] == 1 and y_true[i] == 0:\n",
    "            fp+= 1\n",
    "        elif y_pred[i] == 0 and y_true[i] == 1:\n",
    "            fn+= 1\n",
    "        else:\n",
    "            tp+= 1\n",
    "            score+= 1\n",
    "    score/= l\n",
    "    print('Accuracy: '+str(score))\n",
    "    print('     T       F')\n",
    "    print('P    '+str(tp)+' '*(8-len(str(tp)))+str(fp))\n",
    "    print('N    '+str(tn)+' '*(8-len(str(tn)))+str(fn))\n",
    "    return score\n",
    "\n",
    "def save_model(model, source, name):\n",
    "    os.chdir(source+'\\\\MURA-v1.1\\\\models')\n",
    "    model.save(name+'.h5')\n",
    "    \n",
    "def load_model(source, name):\n",
    "    os.chdir(source+'\\\\MURA-v1.1\\\\models')\n",
    "    return models.load_model(name+'.h5')\n",
    "\n",
    "def copy_model(model):\n",
    "    model_copy = models.clone_model(model)\n",
    "    model_copy.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_copy\n",
    "\n",
    "def class_weight(data_y):\n",
    "    positive = np.sum(data_y)\n",
    "    negative = np.size(data_y) - positive\n",
    "    return {0 : 1 + positive/negative, 1: 1 + negative/positive}\n",
    "\n",
    "def heatmaps(source, kind, reshape, model, last_conv_index, model_name = None, mean = None, std =  None): \n",
    "    image_folder_path = source+'\\\\MURA-v1.1\\\\heatmaps\\\\' + kind + '\\\\images'\n",
    "    for image_name in os.listdir(image_folder_path):\n",
    "        heatmap(source, image_name, kind, reshape, model, last_conv_index, model_name, mean, std)\n",
    "\n",
    "def heatmap(source, image_name, kind, reshape, model, last_conv_index, model_name = None, mean = None, std =  None): # image_name must include extension .png\n",
    "    os.chdir(source+'\\\\MURA-v1.1\\\\heatmaps\\\\'+kind+'\\\\images')\n",
    "    image = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, reshape)\n",
    "    image = np.expand_dims(image, axis = 2)\n",
    "    if (mean, std) != (None, None):\n",
    "        image = (image - mean)/std\n",
    "    pred = round(model.predict(np.array([image])).flatten().tolist()[0])\n",
    "    print('\\''+image_name+'\\''+' predicted to be ', end = '')\n",
    "    if pred == 0:\n",
    "        print('negative')\n",
    "    else:\n",
    "        print('positive')\n",
    "\n",
    "    model_output = model.output\n",
    "    conv_layer = model.get_layer(index = last_conv_index) # indexed from 0\n",
    "    grads = K.gradients(model_output, conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "    iterate = K.function([model.input], [pooled_grads, conv_layer.output[0]])\n",
    "    pooled_grads_val, conv_layer_output_val = iterate(np.array([image]))\n",
    "    for i in range(conv_layer_output_val.shape[2]):\n",
    "        conv_layer_output_val[:, :, i]*= pooled_grads_val[i]\n",
    "    \n",
    "    heatmap = np.mean(conv_layer_output_val, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap/= np.max(heatmap)\n",
    "    image = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap_applied = heatmap * 0.5 + image\n",
    "    os.chdir(source+'\\\\MURA-v1.1\\\\heatmaps\\\\'+kind+'\\\\heatmaps')\n",
    "    image_name = image_name[:-4]\n",
    "    if model_name != None:\n",
    "        cv2.imwrite(model_name +'_'+ image_name +'_heatmap.png', heatmap)\n",
    "        cv2.imwrite(model_name +'_'+ image_name +'_heatmap_applied.png', heatmap_applied)\n",
    "    else:\n",
    "        cv2.imwrite(image_name + 'heatmap.png', heatmap)\n",
    "        cv2.imwrite(image_name + '_heatmap_applied.png', heatmap_applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\python' # depends on where you saved MURA\n",
    "reshape = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: how to extract data\n",
    "train_x, train_y, test_x, test_y = extract_data(source ,'humerus', reshape, False)\n",
    "train_x, train_y = shuffler(train_x, train_y)\n",
    "test_x, test_y = shuffler(test_x, test_y)\n",
    "train_patients, test_patients = extract_data_patients(source ,'humerus', reshape, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6b83ae7b5163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_all_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-659bf2fd8d1a>\u001b[0m in \u001b[0;36mextract_all_data\u001b[1;34m(source, reshape, standardize)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_all_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtrain_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_all_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtest_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_all_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstandardize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-659bf2fd8d1a>\u001b[0m in \u001b[0;36mextract_all_helper\u001b[1;34m(source, torv, reshape)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\MURA-v1.1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorv\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_image_paths.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-659bf2fd8d1a>\u001b[0m in \u001b[0;36mextract\u001b[1;34m(source, file, reshape)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreadCSV\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'positive'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = extract_all_data(source, reshape, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'train_negative1.png' predicted to be negative\n",
      "'train_negative10.png' predicted to be negative\n",
      "'train_negative11.png' predicted to be negative\n",
      "'train_negative12.png' predicted to be negative\n",
      "'train_negative13.png' predicted to be negative\n",
      "'train_negative14.png' predicted to be positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'train_negative15.png' predicted to be negative\n",
      "'train_negative16.png' predicted to be negative\n",
      "'train_negative17.png' predicted to be negative\n",
      "'train_negative18.png' predicted to be negative\n",
      "'train_negative19.png' predicted to be negative\n",
      "'train_negative2.png' predicted to be positive\n",
      "'train_negative20.png' predicted to be negative\n",
      "'train_negative21.png' predicted to be negative\n",
      "'train_negative22.png' predicted to be negative\n",
      "'train_negative23.png' predicted to be negative\n",
      "'train_negative24.png' predicted to be positive\n",
      "'train_negative25.png' predicted to be negative\n",
      "'train_negative26.png' predicted to be negative\n",
      "'train_negative27.png' predicted to be negative\n",
      "'train_negative28.png' predicted to be positive\n",
      "'train_negative29.png' predicted to be negative\n",
      "'train_negative3.png' predicted to be negative\n",
      "'train_negative30.png' predicted to be negative\n",
      "'train_negative4.png' predicted to be negative\n",
      "'train_negative5.png' predicted to be negative\n",
      "'train_negative6.png' predicted to be negative\n",
      "'train_negative7.png' predicted to be negative\n",
      "'train_negative8.png' predicted to be negative\n",
      "'train_negative9.png' predicted to be negative\n",
      "'train_positive1.png' predicted to be negative\n",
      "'train_positive10.png' predicted to be negative\n",
      "'train_positive11.png' predicted to be negative\n",
      "'train_positive12.png' predicted to be negative\n",
      "'train_positive13.png' predicted to be negative\n",
      "'train_positive14.png' predicted to be negative\n",
      "'train_positive15.png' predicted to be positive\n",
      "'train_positive16.png' predicted to be negative\n",
      "'train_positive17.png' predicted to be negative\n",
      "'train_positive18.png' predicted to be negative\n",
      "'train_positive19.png' predicted to be positive\n",
      "'train_positive2.png' predicted to be positive\n",
      "'train_positive20.png' predicted to be negative\n",
      "'train_positive21.png' predicted to be negative\n",
      "'train_positive22.png' predicted to be positive\n",
      "'train_positive23.png' predicted to be positive\n",
      "'train_positive24.png' predicted to be positive\n",
      "'train_positive25.png' predicted to be negative\n",
      "'train_positive26.png' predicted to be negative\n",
      "'train_positive27.png' predicted to be positive\n",
      "'train_positive28.png' predicted to be positive\n",
      "'train_positive29.png' predicted to be negative\n",
      "'train_positive3.png' predicted to be positive\n",
      "'train_positive30.png' predicted to be negative\n",
      "'train_positive4.png' predicted to be negative\n",
      "'train_positive5.png' predicted to be positive\n",
      "'train_positive6.png' predicted to be negative\n",
      "'train_positive7.png' predicted to be positive\n",
      "'train_positive8.png' predicted to be positive\n",
      "'train_positive9.png' predicted to be negative\n",
      "'valid_negative1.png' predicted to be negative\n",
      "'valid_negative10.png' predicted to be negative\n",
      "'valid_negative11.png' predicted to be positive\n",
      "'valid_negative12.png' predicted to be negative\n",
      "'valid_negative13.png' predicted to be negative\n",
      "'valid_negative14.png' predicted to be positive\n",
      "'valid_negative15.png' predicted to be negative\n",
      "'valid_negative16.png' predicted to be negative\n",
      "'valid_negative17.png' predicted to be negative\n",
      "'valid_negative18.png' predicted to be negative\n",
      "'valid_negative19.png' predicted to be negative\n",
      "'valid_negative2.png' predicted to be negative\n",
      "'valid_negative20.png' predicted to be negative\n",
      "'valid_negative21.png' predicted to be negative\n",
      "'valid_negative22.png' predicted to be negative\n",
      "'valid_negative23.png' predicted to be positive\n",
      "'valid_negative24.png' predicted to be negative\n",
      "'valid_negative25.png' predicted to be negative\n",
      "'valid_negative26.png' predicted to be positive\n",
      "'valid_negative27.png' predicted to be negative\n",
      "'valid_negative28.png' predicted to be positive\n",
      "'valid_negative29.png' predicted to be negative\n",
      "'valid_negative3.png' predicted to be negative\n",
      "'valid_negative30.png' predicted to be negative\n",
      "'valid_negative4.png' predicted to be negative\n",
      "'valid_negative5.png' predicted to be negative\n",
      "'valid_negative6.png' predicted to be negative\n",
      "'valid_negative7.png' predicted to be negative\n",
      "'valid_negative8.png' predicted to be negative\n",
      "'valid_negative9.png' predicted to be negative\n",
      "'valid_positive1.png' predicted to be negative\n",
      "'valid_positive10.png' predicted to be negative\n",
      "'valid_positive11.png' predicted to be negative\n",
      "'valid_positive12.png' predicted to be negative\n",
      "'valid_positive13.png' predicted to be negative\n",
      "'valid_positive14.png' predicted to be positive\n",
      "'valid_positive15.png' predicted to be negative\n",
      "'valid_positive16.png' predicted to be negative\n",
      "'valid_positive17.png' predicted to be positive\n",
      "'valid_positive18.png' predicted to be negative\n",
      "'valid_positive19.png' predicted to be positive\n",
      "'valid_positive2.png' predicted to be negative\n",
      "'valid_positive20.png' predicted to be negative\n",
      "'valid_positive21.png' predicted to be negative\n",
      "'valid_positive22.png' predicted to be positive\n",
      "'valid_positive23.png' predicted to be negative\n",
      "'valid_positive24.png' predicted to be negative\n",
      "'valid_positive25.png' predicted to be negative\n",
      "'valid_positive26.png' predicted to be positive\n",
      "'valid_positive27.png' predicted to be negative\n",
      "'valid_positive28.png' predicted to be positive\n",
      "'valid_positive29.png' predicted to be negative\n",
      "'valid_positive3.png' predicted to be negative\n",
      "'valid_positive30.png' predicted to be negative\n",
      "'valid_positive4.png' predicted to be negative\n",
      "'valid_positive5.png' predicted to be negative\n",
      "'valid_positive6.png' predicted to be positive\n",
      "'valid_positive7.png' predicted to be negative\n",
      "'valid_positive8.png' predicted to be negative\n",
      "'valid_positive9.png' predicted to be negative\n"
     ]
    }
   ],
   "source": [
    "# Example: how to use models\n",
    "model = load_model(source, 'conv3HUMERUS')\n",
    "heatmaps(source, 'humerus', (512,512), model, 12, 'conv3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
